{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Combo-NER-usage.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNY3pOc2e8Aa3RhfCnA1qr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexwz/ComboNER/blob/master/Combo_NER_usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY6HZilKLpF-",
        "outputId": "428d11b2-97b2-4a35-e733-ef3409b89e39"
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8th-0BOwjeke"
      },
      "source": [
        "While embedding vectors are a part of the model, BPEMB is required to use the subword tokenizer of appropriate granularity and encode words to subword-IDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loqyhswgP_qq",
        "outputId": "02ddfe2e-7cd6-4910-ce97-ea32b224fc11"
      },
      "source": [
        "!pip install bpemb==0.3.3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bpemb==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/6f/9191b85109772636a8f8accb122900c34db26c091d2793218aa94954524c/bpemb-0.3.3-py3-none-any.whl\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from bpemb==0.3.3) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb==0.3.3) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb==0.3.3) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpemb==0.3.3) (4.41.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb==0.3.3) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb==0.3.3) (5.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb==0.3.3) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb==0.3.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb==0.3.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb==0.3.3) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb==0.3.3) (2.10)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.3 sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWwp16s1CeQO"
      },
      "source": [
        "To de-serialize the model, we use load_model method from TensorFlow 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DWy9LlGspqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b980cef6-af30-4b6a-b9b6-e9697c9e8803"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model( '/content/drive/My Drive/combo/model_final/1model_final/')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqQrYPPLTSdG"
      },
      "source": [
        "model.compile(optimizer='Adamax')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qtd1o-tcDgei",
        "outputId": "d5921dd6-6360-4447-a1a3-e9dd9728363a"
      },
      "source": [
        "from bpemb import BPEmb\n",
        "\n",
        "params = {  'embedding_size': 100, 'vocabulary_size': 50000 }\n",
        "\n",
        "bpemb_pl = BPEmb(lang=\"pl\", dim=params['embedding_size'], vs=params['vocabulary_size'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/pl/pl.wiki.bpe.vs50000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1135152/1135152 [00:01<00:00, 735150.12B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/pl/pl.wiki.bpe.vs50000.d100.w2v.bin.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19000997/19000997 [00:03<00:00, 5193698.12B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ivr4opcWsQg"
      },
      "source": [
        "tokenids = bpemb_pl.encode_ids('Ala ma kota, pięć psów i mieszka na wsi.')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkkqFNDKp66k",
        "outputId": "795fb8bd-8638-4e64-9c3c-f2b053643890"
      },
      "source": [
        "tokenids"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5695, 123, 24167, 49903, 3868, 16843, 28, 1859, 33, 1349, 49902]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkxpH3ykB0n_"
      },
      "source": [
        "sentence_size = len(tokenids)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKQipJWk3Kkm"
      },
      "source": [
        "We need a method to pad the subword IDs to the max length of 31:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44ZSvTDopNW4"
      },
      "source": [
        "def pad_input(tokenids):\n",
        "  return tf.keras.preprocessing.sequence.pad_sequences(tokenids, maxlen=31, padding='post')\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKEI4sSskqjp",
        "outputId": "efb61838-3532-4bd4-f709-9e308765b5a0"
      },
      "source": [
        "pad_input([tokenids])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5695,   123, 24167, 49903,  3868, 16843,    28,  1859,    33,\n",
              "         1349, 49902,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6o1VLxz2UTI"
      },
      "source": [
        "The easiest way of using Combo-NER is via TensorFlow's predict API. For each input sentence, the model returns four lists of predictions:\n",
        "\n",
        "*   part-of-speech\n",
        "*   dependency heads\n",
        "*   dependency relation labels\n",
        "*   named entities\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy3YGBjkQdbn"
      },
      "source": [
        "y_pos, y_heads, y_deprels, y_namedents = model.predict( pad_input([tokenids]) )"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk9Iz6qe2xvP"
      },
      "source": [
        "For each subword token and each class, the model returns its probability. To obtain the label ID, we have to apply argmax:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2sbr12hqAYs"
      },
      "source": [
        "y_pos = tf.argmax(y_pos, -1)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXSFMN__3727"
      },
      "source": [
        "Now, each subword has it's own class label as an integer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-oGn9-fkQwk",
        "outputId": "89a6f877-bce0-4b35-cba6-f3fdac3d5a31"
      },
      "source": [
        "y_pos"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 31), dtype=int64, numpy=\n",
              "array([[11, 15,  7, 15,  8,  7,  4,  7,  1,  7,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNI1CQDt4FHy"
      },
      "source": [
        "To obtain string labels, we need to load sklearn's label encoders, available from http://mozart.ipipan.waw.pl/~axw/Combo-NER/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDFqzRzGfj4b"
      },
      "source": [
        "import pickle\n",
        "\n",
        "le_ner = pickle.load(open('/content/drive/My Drive/combo/le_ner.pkl', 'rb'))\n",
        "le_upostag = pickle.load(open('/content/drive/My Drive/combo/le_upostag.pkl', 'rb'))\n",
        "le_deprel = pickle.load(open('/content/drive/My Drive/combo/le_deprel.pkl', 'rb'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUpEquARB8EG"
      },
      "source": [
        "Now let's print POS, remembering to pad to sentence size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8eWTMxBfnQB",
        "outputId": "f7da5e96-ac12-4ad4-b210-43cfc847dab4"
      },
      "source": [
        "list(le_upostag.inverse_transform(y_pos[0][:sentence_size-1]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PROPN',\n",
              " 'VERB',\n",
              " 'NOUN',\n",
              " 'VERB',\n",
              " 'NUM',\n",
              " 'NOUN',\n",
              " 'CCONJ',\n",
              " 'NOUN',\n",
              " 'ADP',\n",
              " 'NOUN',\n",
              " 'ADJ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylq14neZCFeA"
      },
      "source": [
        "Other output types (dependency, named entities) can be printed in the same manner."
      ]
    }
  ]
}